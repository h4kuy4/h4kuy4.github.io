<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Archlinux下搭建pytorch环境</title>
      <link href="/2021/10/08/build-pytorch-environment/"/>
      <url>/2021/10/08/build-pytorch-environment/</url>
      
        <content type="html"><![CDATA[<h2 id="安装Python"><a class="header-anchor" href="#安装Python"></a>安装Python</h2><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ <span class="token function">sudo</span> pacman -S python<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>安装好后执行</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ python -V<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>若输出python版本信息，则表示安装成功</p><h2 id="安装nvidia和cuda驱动"><a class="header-anchor" href="#安装nvidia和cuda驱动"></a>安装nvidia和cuda驱动</h2><h3 id="安装nvidia驱动"><a class="header-anchor" href="#安装nvidia驱动"></a>安装nvidia驱动</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ <span class="token function">sudo</span> pacman -S nvidia-dkms nvidia-settings nvidia-utils<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>安装好后重启，再执行</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ nvidia-smi<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>若输出以下内容则表示显卡驱动安装成功</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">+-----------------------------------------------------------------------------+<span class="token operator">|</span> NVIDIA-SMI <span class="token number">470.74</span>       Driver Version: <span class="token number">470.74</span>       CUDA Version: <span class="token number">11.4</span>     <span class="token operator">|</span><span class="token operator">|</span>-------------------------------+----------------------+----------------------+<span class="token operator">|</span> GPU  Name        Persistence-M<span class="token operator">|</span> Bus-Id        Disp.A <span class="token operator">|</span> Volatile Uncorr. ECC <span class="token operator">|</span><span class="token operator">|</span> Fan  Temp  Perf  Pwr:Usage/Cap<span class="token operator">|</span>         Memory-Usage <span class="token operator">|</span> GPU-Util  Compute M. <span class="token operator">|</span><span class="token operator">|</span>                               <span class="token operator">|</span>                      <span class="token operator">|</span>               MIG M. <span class="token operator">|</span><span class="token operator">|</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span><span class="token operator">+=</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span><span class="token operator">+=</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span><span class="token operator">|</span><span class="token operator">|</span>   <span class="token number">0</span>  NVIDIA GeForce <span class="token punctuation">..</span>.  Off  <span class="token operator">|</span> 00000000:01:00.0 Off <span class="token operator">|</span>                  N/A <span class="token operator">|</span><span class="token operator">|</span> N/A   44C    P8    N/A /  N/A <span class="token operator">|</span>      4MiB /  4042MiB <span class="token operator">|</span>      <span class="token number">0</span>%      Default <span class="token operator">|</span><span class="token operator">|</span>                               <span class="token operator">|</span>                      <span class="token operator">|</span>                  N/A <span class="token operator">|</span>+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+<span class="token operator">|</span> Processes:                                                                  <span class="token operator">|</span><span class="token operator">|</span>  GPU   GI   CI        PID   Type   Process name                  GPU Memory <span class="token operator">|</span><span class="token operator">|</span>        ID   ID                                                   Usage      <span class="token operator">|</span><span class="token operator">|</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span><span class="token operator">|</span><span class="token operator">|</span>    <span class="token number">0</span>   N/A  N/A       <span class="token number">511</span>      G   /usr/lib/Xorg                       4MiB <span class="token operator">|</span>+-----------------------------------------------------------------------------+<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>注意，部分笔记本为独显渲染图形，然后由核显输出到自带的屏幕上，这些笔记本安装到这一步仅仅只能调用cuda，而无法渲染显示输出</strong></p><h3 id="安装cuda驱动"><a class="header-anchor" href="#安装cuda驱动"></a>安装cuda驱动</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ <span class="token function">sudo</span> pacman -S cuda cudnn<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="安装Pytorch"><a class="header-anchor" href="#安装Pytorch"></a>安装Pytorch</h2><p>Arch官方提供了4种pytorch包</p><table><thead><tr><th style="text-align:left">包名</th><th style="text-align:left">包的特点</th></tr></thead><tbody><tr><td style="text-align:left"><code>python-pytorch</code></td><td style="text-align:left">可调用CPU和OpenCL运算</td></tr><tr><td style="text-align:left"><code>python-pytorch-cuda</code></td><td style="text-align:left">可调用CPU，CUDA和OpenCL</td></tr><tr><td style="text-align:left"><code>python-pytorch-opt</code></td><td style="text-align:left">只可调用OpenCL运算</td></tr><tr><td style="text-align:left"><code>python-pytorch-opt-cuda</code></td><td style="text-align:left">可调用CUDA和OpenCL</td></tr></tbody></table><p>这里选用<code>python-pytorch-cuda</code>包<br>执行安装命令</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ <span class="token function">sudo</span> pacman -S python-pytorch-cuda<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>安装好后进入python的IDLE环境</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ pythonPython <span class="token number">3.9</span>.7 <span class="token punctuation">(</span>default, Aug <span class="token number">31</span> <span class="token number">2021</span>, <span class="token number">13</span>:28:12<span class="token punctuation">)</span><span class="token punctuation">[</span>GCC <span class="token number">11.1</span>.0<span class="token punctuation">]</span> on linuxType <span class="token string">"help"</span>, <span class="token string">"copyright"</span>, <span class="token string">"credits"</span> or <span class="token string">"license"</span> <span class="token keyword">for</span> <span class="token function">more</span> information.<span class="token operator">>></span><span class="token operator">></span> <span class="token function">import</span> torch<span class="token operator">>></span><span class="token operator">></span> torch.cuda.is_available<span class="token punctuation">(</span><span class="token punctuation">)</span>True<span class="token operator">>></span><span class="token operator">></span> from torch.backends <span class="token function">import</span> cudnn<span class="token operator">>></span><span class="token operator">></span> cudnn.is_available<span class="token punctuation">(</span><span class="token punctuation">)</span>True<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>若<code>torch.cuda.is_available()</code>和<code>cudnn.is_available()</code>输出均为<code>True</code>则Pytorch环境安装成功</p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Archlinux </tag>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
